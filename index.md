---
layout: default
title: Blog Post
tagline: Audio-Visual Speech Separation
description: Blog post for EE 380L Data Mining Project
---

* \[Header Picture\]

<p align="justify">
 
# Speech Separation using Audio-Visual Features
Alexander Fiore, Alexander Phillips, Arivarasi Kesawaram, Dae Yeol Lee, Dan Jacobellis

</p>

[comment]: # (Abstract: 1-2 Paragraphs)

The visual component of human speech provides rich information about the acoustic signal that can be exploited for source separation. For our project, we train a model to estimate the time-frequency audio mask that separates two speakers in a single channel recording that utilizes the visual information of the speech. Over 5000 videos were retreived from the [AVspeech][2] dataset and combined into pairs to form speech mixtures and ground truth labels. To our knowledge, this project is the first publicly available end-to-end implementation of the [state of the art speech separation model developed by google research][1].

[1]:https://looking-to-listen.github.io
[2]:https://looking-to-listen.github.io/avspeech/

* \[Introduction & Background (all items should be high level overviews)\]
  * \[Problem being addressed and why it’s important\]
  * \[Related work\]
  * \[Outline of approach and rationale (high level)\]
  * \[Contributions or novel characteristics of project\]
* \[Data Collection/Description\]
  * \[Relevant Characteristics\]
  * \[Source(s)\]
  * \[Methods of acquisition\]
* \[Data Pre-Processing & Exploration\]
  * \[Feature engineering/selection\]
  * \[Relevant Plots\]
* \[Learning/Modeling\]
  * \[Chosen models and why\]
  * \[Training methods (validation, parameter selection)\]
  * \[Other design choices\]
* \[Results\]
  * \[Key findings and evaluation\]
  * \[Comparisons from different approaches\]
  * \[Plots and figures\]
* \[Conclusion\]
  * \[Summarize everything above\]
  * \[Lessons learned\]
  * \[Future work - continuations or improvements\]
* \[References\]
* \[Relevant project links (i.e. Github, Bitbucket, etc…)\]
