---
layout: default
title: Blog Post
tagline: Speech Separation using Audio-Visual Features
description: Blog post for EE 380L Data Mining Project
---
<table style="border:none"><td align="center" style="border:none"> 
<img src="img/cocktail.jpg"></td></table>

***Alexander Fiore, Alexander Phillips, Arivarasi Kesawaram, Dae Yeol Lee, Dan Jacobellis***

[comment]: # (Abstract: 1-2 Paragraphs)

The visual component of human speech provides rich information about the acoustic signal that can be exploited for source separation. For our project, we train a model to estimate the time-frequency audio mask that separates two speakers in a single channel recording that utilizes the visual information of the speech. Over 5000 videos were retrieved from the [AVspeech][2] dataset and combined into pairs to form speech mixtures and ground truth labels. To our knowledge, this project is the first publicly available end-to-end implementation of the [state of the art speech separation model developed by google research][1].

[comment]: # (Introduction & Background / Problem being addressed and why it’s important / Related work / approach and rationale / contribution or novel characteristics)

# Audio separation and the visual information of speech
The binaural processing of the human auditory system in combination with visual cues allows us to effectively focus our attention on a single speaker when multiple speakers and other noise are present. When audio containing multiple speakers is recorded digitally and combined into a single channel (e.g. the recent presidential debates), the visual and spatial component of the acoustic field is lost, and it becomes much harder for a listener to understand the speakers individually. 

The exact mechanisms of the human auditory system to separate speech (the cocktail party effect) are not fully understood and developing computational methods to replicate it remains an open problem. State of the art methods that use audio only, also known as blind separation methods, still leave much to be desired, which motivates the use of secondary information, either visual or spatial. Since recorded audio is increasingly accompanied by video (but not by multi-channel audio), exploting visual information is more convenient and useful. Use of the visual signal is further motivated further by its known power, as evidenced by human lip reading.

# Related Work
To our knowledge, the current state of the art for audio-visual separation is a [deep network model developed by google research][1] which was later accompanied by the [AVSpeech][2] dataset. While its results are impressive, the implementation is proprietary and many of the details of the processing pipeline are left ambiguous. Regardless, it provides an excellent learning model architecture and introduces several novel techniques which we have adopted. To our knowledge, our implementation is the first publicly available, end-to-end audio-visual speech separation model.

# Overview of architecture, data pipeline, and results
The model explored in our project is a deep neural network (DNN) trained using face embeddings
generated by the [FaceNet][3] model and the mixture of audio streams from two videos. The output of the model is a time-frequency audio mask to be used for audio source separation. The adopted audio-visual network architecture consists of both convolutional and recurrent layers. 

The visual pre-processing consists of (1) a face detection model, (2) rejection of videos with occluded faces (3) resampling of cropped faces to a uniform size, and (4) per-frame application of the FaceNet model to create a sequence of face embeddings. The audio pre-processing consists of (1) mixing the audio from a pairs of videos containing speech, (2) applying a time-frequency transform, and (3) constructing the ideal mask to use as the ground truth in training.

YouTube videos referenced by URLs in the AVspeech dataset were downloaded and preprocessed in pairs to generate training data corresponding to a two-speaker scenario. The final model was trained on approximately 2500 mixtures from 5000 YouTube videos containing unique speakers

A variant of the structural similarity measure used in image quality assessment was used as the loss between the predicted and ideal audio masks during the training. The network was trained using a consumer GPU on a personal workstation iterating over the full dataset in several epochs. It produces speech separation masks which show some similarity to the ideal masks, but the error remains quite high judging by listening to the reconstructed audio streams.

* \[Data Collection/Description\]
  * \[Relevant Characteristics\]
  * \[Source(s)\]
  * \[Methods of acquisition\]
* \[Data Pre-Processing & Exploration\]
  * \[Feature engineering/selection\]
  * \[Relevant Plots\]
* \[Learning/Modeling\]
  * \[Chosen models and why\]
  * \[Training methods (validation, parameter selection)\]
  * \[Other design choices\]
* \[Results\]
  * \[Key findings and evaluation\]
  * \[Comparisons from different approaches\]
  * \[Plots and figures\]
* \[Conclusion\]
  * \[Summarize everything above\]
  * \[Lessons learned\]
  * \[Future work - continuations or improvements\]
* \[References\]
* \[Relevant project links (i.e. Github, Bitbucket, etc…)\]


[1]:https://looking-to-listen.github.io
[2]:https://looking-to-listen.github.io/avspeech/
[3]:https://github.com/davidsandberg/facenet
